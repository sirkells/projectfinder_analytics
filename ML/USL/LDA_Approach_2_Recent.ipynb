{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import relevant libraries and download relevant resources\n",
    "2. Obtain the details for retrieving and stroing the data\n",
    "3. Get the dataset from mongoDB database and store it as a pandas dataframe. <br/>\n",
    "4. Reduce the orginial dataframe by removing the columns which are not needed for Topic Modelling. Cureently we are considering only the area and the description of the project as the columns in our dataframe. \n",
    "5. Identifying stopwords:\n",
    "    5.1. Load NLTK's English and German stopwords\n",
    "    5.2. Add cities and mothns to it \n",
    "    5.3. Manually added stopwords (irrelevant words for our analysis)\n",
    "6. Creation of Stemmer: Creating our own stemmer as a dictionary where we specify how to combine same words\n",
    "7. Tokenzing and Stemming the description data present in the dataframe.\n",
    "8. Get frequency distribution of all words present in the dataframe.\n",
    "9. Choose a threshold frequency for top words(currently 100)\n",
    "10. Reduce the words in tokenized column to these top words for each row\n",
    "11. Remove the rows which have less than 10 tokens\n",
    "12. Create training and test dataset\n",
    "13. Train LDA model\n",
    "14. Save the model as 'LDA_Approach_1.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the relevant libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import string\n",
    "from string import punctuation\n",
    "import os\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the current directory where this notebook is present as working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currDir = os.getcwd()\n",
    "if \"USL\" not in currDir:\n",
    "    os.chdir(os.path.join(currDir,  \"ML\", \"USL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading relevant resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using NLTK Downloader to obtain the resource stopwords, punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database details for retrieving dataset and storing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Details for retrieving  data from projectfinder\n",
    "db_loc = {\n",
    "    'ip' :'10.10.250.0',\n",
    "    'port' : 27017,\n",
    "    'database' : 'projectfinder',\n",
    "    'collection' : 'itproject_clean'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#Details for storing data related to projectfinder\n",
    "db_data = {\n",
    "    'ip' :'10.10.250.0',\n",
    "    'port' : 27017,\n",
    "    'database' : 'projectfinder',\n",
    "    'collection' : 'mldata1'\n",
    "}\n",
    "\n",
    "#%%[markdown]\n",
    "#Methods for loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a path for the directory containing constants and some self made packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = os.path.join(currDir,  \"constants\")\n",
    "import constants.load_save_dataset as load_save_DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from mongodb itproject_clean collection succesfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14059, 25)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rawData = load_save_DS.load_dataset_from_mongodb(db_loc)\n",
    "df_rawData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_required_dataset(original_dataset):\n",
    "    \n",
    "    #Select required colunms\n",
    "    df = original_dataset[['description', 'bereich']]\n",
    "    df = df[df['description'] != '']\n",
    "    #df.rename(columns = {'description' : 'project', 'bereich' : 'class'})\n",
    "    df['project'] = df['description']\n",
    "    df['label'] = df['bereich']\n",
    "    df.drop(['description', 'bereich'], axis=1, inplace=True)\n",
    "    df = df[df['label'] != 'IT/Bauingenieur']\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Für einen unserer Kunden aus dem Finanzdienstl...</td>\n",
       "      <td>Infr-Admin-Microsoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kann Profil leider nicht löschen.</td>\n",
       "      <td>IT/Consulting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Intelligence Analyst (m/w) - Tableau ...</td>\n",
       "      <td>Data-Sci-BI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Konzeption, Customizing sowie Softwareanpassun...</td>\n",
       "      <td>Infr-Admin-Linux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es sollen mehrere Automatisierungen mit ubot S...</td>\n",
       "      <td>IT/IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             project                 label\n",
       "0  Für einen unserer Kunden aus dem Finanzdienstl...  Infr-Admin-Microsoft\n",
       "1                  Kann Profil leider nicht löschen.         IT/Consulting\n",
       "2  Business Intelligence Analyst (m/w) - Tableau ...           Data-Sci-BI\n",
       "3  Konzeption, Customizing sowie Softwareanpassun...      Infr-Admin-Linux\n",
       "4  Es sollen mehrere Automatisierungen mit ubot S...                 IT/IT"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessedDataset = get_required_dataset(df_rawData)\n",
    "df_preprocessedDataset.shape\n",
    "df_preprocessedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aufgabe: \\n* Erstellen von Layouts zur Entwick...</td>\n",
       "      <td>IT-Technical-Dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Folgende Qualifikationen / Aufgabenstellungen ...</td>\n",
       "      <td>Infr-Admin-Database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Für unseren Kunden suchen wir aktuell eine/n \\...</td>\n",
       "      <td>Infr-Database-Admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beginn/Dauer:  6 PM mit Option \\r\\nEinsatzort:...</td>\n",
       "      <td>Dev-Web-Fullstack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Sie begleiten das CRM-System auf der IT-Entw...</td>\n",
       "      <td>IT/ERP / CRM Sy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             project                label\n",
       "0  Aufgabe: \\n* Erstellen von Layouts zur Entwick...     IT-Technical-Dev\n",
       "1  Folgende Qualifikationen / Aufgabenstellungen ...  Infr-Admin-Database\n",
       "2  Für unseren Kunden suchen wir aktuell eine/n \\...  Infr-Database-Admin\n",
       "3  Beginn/Dauer:  6 PM mit Option \\r\\nEinsatzort:...    Dev-Web-Fullstack\n",
       "4  - Sie begleiten das CRM-System auf der IT-Entw...      IT/ERP / CRM Sy"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the data\n",
    "df_preprocessedDataset = df_preprocessedDataset.sample(frac=1.0)\n",
    "df_preprocessedDataset.reset_index(drop=True,inplace=True)\n",
    "df_preprocessedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aufgabe: \\n* Erstellen von Layouts zur Entwicklung neuer Mainboards, Platinen sowie Leiterplatten für Sondermaschinenbau \\n \\nVoraussetzungen: \\n* Erfahrung in der Layouterstellung mit EAGLE \\n \\nEintrittsdatum: 01.09.2018 \\nDauer: 24 Wochen'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessedDataset.iloc[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's German and english stopwords'\n",
    "dataDir = os.path.join(currDir,  \"constants\")\n",
    "with open(os.path.join(dataDir, 'german_stopwords_full.txt'), 'r') as f:\n",
    "    stopwords_germ = f.read().splitlines()\n",
    "stopwords_eng = nltk.corpus.stopwords.words('english')\n",
    "combined_stopWordsPath = os.path.join(dataDir, 'stopwords_manual.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#german cities\n",
    "from constants.bundeslander import Baden_Württemberg, Bayern, Berlin, Brandenburg, Bremen, Hamburg, Hessen, Mecklenburg_Vorpommern, Niedersachsen, Nordrhein_Westfalen, Rheinland_Pfalz, Saarland, Sachsen, Sachsen_Anhalt, Schleswig_Holstein, Thüringen, Ausland\n",
    "\n",
    "All = Baden_Württemberg + Bayern + Berlin + Brandenburg + Bremen +Hamburg + Hessen + Mecklenburg_Vorpommern + Niedersachsen + Nordrhein_Westfalen + Rheinland_Pfalz + Saarland + Sachsen + Sachsen_Anhalt + Schleswig_Holstein + Thüringen + Ausland\n",
    "cities = list(set([city.lower() for city in All]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['januar', 'january', 'februar', 'february', 'märz', 'march', 'april', 'mai', 'may', 'juni', 'june', 'juli', 'july', 'august', 'september', 'oktober', 'october', 'november', 'dezember', 'december']\n"
     ]
    }
   ],
   "source": [
    "months = ['Januar', 'January','Februar', 'February', 'März', 'March', 'April', 'Mai', 'May', 'Juni', 'June', 'Juli', \n",
    "          'July', 'August', 'September', 'Oktober', 'October', 'November', 'Dezember', 'December']\n",
    "months = [month.lower() for month in months]\n",
    "print(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n"
     ]
    }
   ],
   "source": [
    "stopwords_manual = [line.rstrip('\\n') for line in open(combined_stopWordsPath)]\n",
    "print(len(stopwords_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13240"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_all = list(set(stopwords_germ + stopwords_eng + stopwords_manual + cities + months))\n",
    "len(stopwords_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "stopwords_add = []\n",
    "stopwords_add = list(set(stopwords_add + stopwords_manual))\n",
    "checker = list(set(stopwords_germ + stopwords_eng + cities + months))\n",
    "stopwords_add.sort()\n",
    "with open(combined_stopWordsPath, 'w') as f:\n",
    "    for item in stopwords_add:\n",
    "        if item not in checker:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13240"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_manual = [line.rstrip('\\n') for line in open(os.path.join(dataDir, combined_stopWordsPath))]\n",
    "print(len(stopwords_manual))\n",
    "\n",
    "stopwords_all = list(set(stopwords_germ + stopwords_eng + stopwords_manual + cities + months))\n",
    "len(stopwords_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmerLocation = os.path.join(dataDir, \"stemmer_own.pickle\")\n",
    "pickle_in = open(stemmerLocation,\"rb\")\n",
    "stemmer_own = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform tokenization, stemming on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):\n",
    "    \"\"\"Normalize, tokenize, stem the original text string\n",
    "    \n",
    "    Args:\n",
    "    text: string. String containing message for processing\n",
    "       \n",
    "    Returns:\n",
    "    cleaned: list of strings. List containing normalized and stemmed word tokens with bigrams\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        text = re.sub(r'(\\d)',' ',text.lower())\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens_cleaned = [word for word in tokens if word not in stopwords_all and len(word) > 1]\n",
    "        \n",
    "        stemmed_tokens = []\n",
    "        stemmer_keys = list(stemmer_own.keys())\n",
    "        for word in tokens_cleaned:\n",
    "            for stemmer_key in stemmer_keys:\n",
    "                if stemmer_key in word:\n",
    "                    stemmed_word = stemmer_own[stemmer_key]\n",
    "                    stemmed_tokens.append(stemmed_word)\n",
    "                    break\n",
    "            else:\n",
    "                stemmed_tokens.append(word)\n",
    "  \n",
    "                \n",
    "\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text and title and create new column \"tokenized\"\n",
    "t1 = time.time()\n",
    "df_preprocessedDataset['token_stem_spRm'] = df_preprocessedDataset['project'].apply(text_processing)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to prepare 12168 projects documents: 9.076289677619934 min\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken to prepare\", len(df_preprocessedDataset), \"projects documents:\", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>label</th>\n",
       "      <th>token_stem_spRm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aufgabe: \\n* Erstellen von Layouts zur Entwick...</td>\n",
       "      <td>IT-Technical-Dev</td>\n",
       "      <td>[layouts, entwicklung, mainboards, platinen, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Folgende Qualifikationen / Aufgabenstellungen ...</td>\n",
       "      <td>Infr-Admin-Database</td>\n",
       "      <td>[aufgabenstellungen, tiefes, entwicklung, db, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Für unseren Kunden suchen wir aktuell eine/n \\...</td>\n",
       "      <td>Infr-Database-Admin</td>\n",
       "      <td>[architekt, lamp, informatik, arbeitszeit, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beginn/Dauer:  6 PM mit Option \\r\\nEinsatzort:...</td>\n",
       "      <td>Dev-Web-Fullstack</td>\n",
       "      <td>[pm, entwicklung, testvorgehens, concept, sctm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Sie begleiten das CRM-System auf der IT-Entw...</td>\n",
       "      <td>IT/ERP / CRM Sy</td>\n",
       "      <td>[begleiten, crm, system, entwicklung, funktion...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             project                label  \\\n",
       "0  Aufgabe: \\n* Erstellen von Layouts zur Entwick...     IT-Technical-Dev   \n",
       "1  Folgende Qualifikationen / Aufgabenstellungen ...  Infr-Admin-Database   \n",
       "2  Für unseren Kunden suchen wir aktuell eine/n \\...  Infr-Database-Admin   \n",
       "3  Beginn/Dauer:  6 PM mit Option \\r\\nEinsatzort:...    Dev-Web-Fullstack   \n",
       "4  - Sie begleiten das CRM-System auf der IT-Entw...      IT/ERP / CRM Sy   \n",
       "\n",
       "                                     token_stem_spRm  \n",
       "0  [layouts, entwicklung, mainboards, platinen, l...  \n",
       "1  [aufgabenstellungen, tiefes, entwicklung, db, ...  \n",
       "2  [architekt, lamp, informatik, arbeitszeit, pro...  \n",
       "3  [pm, entwicklung, testvorgehens, concept, sctm...  \n",
       "4  [begleiten, crm, system, entwicklung, funktion...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(df_preprocessedDataset['token_stem_spRm'].tolist(), min_count=2, threshold=2) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "def make_bigrams(text):\n",
    "    return bigram_mod[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "df_preprocessedDataset['token_stem_spRm_bigram'] = df_preprocessedDataset['token_stem_spRm'].apply(make_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify most frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list containing all the words in a dataframe\n",
    "all_words_df = [word for item in list(df_preprocessedDataset['tokenized']) for word in item]\n",
    "\n",
    "# Use nltk fdist to get a frequency distribution of all words\n",
    "fdist_words = FreqDist(all_words_df)\n",
    "print(len(fdist_words)) # number of unique words\n",
    "print(type(fdist_words))\n",
    "\n",
    "#print(fdist_words.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unique_words = len(fdist_words)\n",
    "sorted_freqDist_words = fdist_words.most_common()\n",
    "maxFreq = sorted_freqDist_words[0][1]\n",
    "print(maxFreq)\n",
    "freq_values = [sorted_freqDist_words[i][1] for i in range(total_unique_words)]\n",
    "avgFreq = np.mean(freq_values)\n",
    "print(avgFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering words with frequency of 100 or more\n",
    "top_words = [sorted_freqDist_words[i][0] for i in range(total_unique_words) if sorted_freqDist_words[i][1] >= 100]\n",
    "print(len(top_words))\n",
    "#print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def most_appeared(text):\n",
    "    return [word for word in text if word in top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the words in tokenized column to the words with frequency more than 100. \n",
    "df_preprocessedDataset['tokenized'] = df_preprocessedDataset['tokenized'].apply(most_appeared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessedDataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep articles with more than 10 tokens, otherwise too short\n",
    "df_preprocessedDataset = df_preprocessedDataset[df_preprocessedDataset['tokenized'].map(len) >= 10]\n",
    "# make sure all tokenized items are lists\n",
    "df_preprocessedDataset = df_preprocessedDataset[df_preprocessedDataset['tokenized'].map(type) == list]\n",
    "df_preprocessedDataset.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df_preprocessedDataset), \"articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask of binary values to split into train and test\n",
    "msk = np.random.rand(len(df_preprocessedDataset)) < 0.9960\n",
    "msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_preprocessedDataset[msk]\n",
    "train_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "test_df = df_preprocessedDataset[~msk]\n",
    "test_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_lda(data, n=10):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize\n",
    "    \"\"\"\n",
    "    num_topics = n\n",
    "    chunksize = 300\n",
    "    dictionary = corpora.Dictionary(data['tokenized'])\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=2)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on \", len(df_preprocessedDataset), \"documents: \", (t2-t1)/60, \"min\")\n",
    "    return dictionary,corpus,lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary,corpus,lda = train_lda(train_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.save('LDA_Approach_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "model =  models.LdaModel.load('LDA_Approach_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all topics\n",
    "model.show_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary_LDA_A1', 'wb') as output:\n",
    "    pickle.dump(dictionary, output)\n",
    "    \n",
    "with open('corpus_LDA_A1', 'wb') as output:\n",
    "    pickle.dump(corpus, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath(\"model\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "pickle.dump(lda, open('model_LDA_A1', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a potentially pretrained model from disk.\n",
    "lda2 = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_topics method shows the the top num_words contributing to num_topics number of random topics\n",
    "lda.show_topics(num_topics=13, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_id in range (2):\n",
    "    print(\"TopicID: \" + str(t_id))\n",
    "    topics = lda.show_topic(topicid=t_id, topn=20)\n",
    "    for topic in topics:\n",
    "        print(topic[0] + \": \" + str(topic[1]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random project from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select an article at random from train_df\n",
    "random_index = int(np.random.randint(len(train_df), size=[1, 1]))\n",
    "print(random_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_check = train_df.iloc[random_index,2]\n",
    "bow = dictionary.doc2bow(data_to_check)\n",
    "doc_distribution = np.array([topic[1] for topic in lda.get_document_topics(bow=bow)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.iloc[random_index,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_distribution)\n",
    "print(len(doc_distribution))\n",
    "np.argsort(-doc_distribution)[:3]\n",
    "print(doc_distribution)\n",
    "print(len(doc_distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of topic distribution for this document\n",
    "def plot_topic_dist(doc_distr, index):\n",
    "    \"\"\"\n",
    "    This function plots the topic distrubtion for a given document\n",
    "    It takes two parameters\n",
    "    (1) doc_distr = type: list of floats, list of topic probability distribution in a document\n",
    "    (2) index = type: int, index number of document to plot\n",
    "    We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12,8));\n",
    "    # the histogram of the data\n",
    "    patches = ax.bar(np.arange(len(doc_distr)), doc_distr)\n",
    "    ax.set_xlabel('Topic ID', fontsize=15)\n",
    "    ax.set_ylabel('Topic Probability Score', fontsize=15)\n",
    "    ax.set_title(\"Topic Distribution for Project in Index \" + str(index), fontsize=20)\n",
    "    ax.set_xticks(range(0,10))\n",
    "    x_ticks_labels = ['ERP/SAP','SW_Dev/Web','IT_App_Mgr/SW_Dev_Arch','SW_Dev/DevOps','Sys_Admin/Support', 'IT_Admin_SW/Oracle/Ops','Data/Ops','IT_Process_Mgr/Consultant', 'MS_DEV/Admin','Business_Analyst/Consulting']\n",
    "    ax.set_xticklabels(x_ticks_labels, rotation='vertical', fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_dist(doc_distribution, random_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  models.LdaModel.load('LDA_Approach_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_distribution1 = np.array([topic[1] for topic in lda_model.get_document_topics(bow=bow)])\n",
    "labels = np.argmax(doc_distribution1)\n",
    "print(doc_distribution1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
