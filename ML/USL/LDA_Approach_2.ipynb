{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import relevant libraries and download relevant resources\n",
    "2. Obtain the details for retrieving and stroing the data\n",
    "3. Get the dataset from mongoDB database and store it as a pandas dataframe. <br/>\n",
    "4. uce the orginial dataframe by removing the columns which are not needed for Topic Modelling. Cureently we are considering only the area and the description of the project as the columns in our dataframe. \n",
    "5. Perform tokenization by removing spaces and punctuations\n",
    "6. Identifying stopwords:\n",
    "    1. Load NLTK's English and German stopwords\n",
    "    2. Add cities and mothns to it \n",
    "    3. Manually added stopwords (irrelevant words for our analysis)\n",
    "7. Create a new column having tokens without stopwords\n",
    "8. Generate bigrams from tokens containing stopwords and apply the bigrams to the tokens without stopwords.\n",
    "9. Store the new tokens with bigrams in a separate column.\n",
    "10. Store all bigrams into  file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the relevant libraries and downloading all relevant resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import string\n",
    "from string import punctuation\n",
    "import os\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using NLTK Downloader to obtain the resource stopwords, punkt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database details for retrieving dataset and storing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Details for retrieving  data from projectfinder\n",
    "db_loc = {\n",
    "    'ip' :'10.10.250.0',\n",
    "    'port' : 27017,\n",
    "    'database' : 'projectfinder',\n",
    "    'collection' : 'itproject_clean'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#Details for storing data related to projectfinder\n",
    "db_data = {\n",
    "    'ip' :'10.10.250.0',\n",
    "    'port' : 27017,\n",
    "    'database' : 'projectfinder',\n",
    "    'collection' : 'mldata1'\n",
    "}\n",
    "\n",
    "#%%[markdown]\n",
    "#Methods for loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_dataset_from_mongodb(db_obj):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method loads a dataset as a pandas dataframe from MongoDB \n",
    "    \n",
    "    Parameters:\n",
    "    @db_obj (dict): Storing the ip address, port number, database name and collection name for dataset to be loaded\n",
    "    \n",
    "    Returns:\n",
    "    panadas dataframe: Containing the loaded dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    #Extracting the items from the inputted dictionary\n",
    "    dbname = db_obj['database']\n",
    "    ip = db_obj['ip']\n",
    "    port = db_obj['port']\n",
    "    collection = db_obj['collection']\n",
    "    \n",
    "    #Creating a connection to the database using MongoClient\n",
    "    connection = MongoClient(ip, port)\n",
    "    db = connection[dbname]\n",
    "    \n",
    "    #Excluding the fileds which are not needed in the dataframe \n",
    "    #Currenlty excluding the id associated with each document of the collection\n",
    "    \n",
    "    exclude_field = {'_id': False}\n",
    "    raw_dataset = list(db[collection].find({}, projection=exclude_field))\n",
    "    \n",
    "    dataset = pd.DataFrame(raw_dataset)\n",
    "    print(f'Data loaded from mongodb {collection} collection succesfully')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_momgodb(df,db_):\n",
    "    \n",
    "    \"\"\"\n",
    "    This method saves a dataframe as a collection into a specified MongoDB database.\n",
    "    \n",
    "    Parameters:\n",
    "    @df (pandas dataframe): Storing the dataset to be saved\n",
    "    @db_ (dict): Details for the database where the given dataset is to be saved\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Convert data prsent in the dataframe to JSON format\n",
    "    data = df.to_dict(orient='records')\n",
    "    \n",
    "     #Extracting the items from the inputted dictionary of database details\n",
    "    dbname = db_['database']\n",
    "    ip = db_['ip']\n",
    "    port = db_['port']\n",
    "    coll = db_['collection']\n",
    "    \n",
    "    #Creating a connection to the database using MongoClient\n",
    "    connection = MongoClient(ip,port)\n",
    "    db = connection[dbname]\n",
    "    col = db[collection].insert_many(data)\n",
    "    \n",
    "    print(f'data saved as {coll}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_dataset_from_json(data):\n",
    "    with open(data) as f:\n",
    "            d = json.load(f)\n",
    "        #normalize json\n",
    "    dataset= json_normalize(d)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from mongodb itproject_clean collection succesfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14059, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rawData = load_dataset_from_mongodb(db_loc)\n",
    "df_rawData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_required_dataset(original_dataset):\n",
    "    \n",
    "    #Select required colunms\n",
    "    df = original_dataset[['description', 'bereich']]\n",
    "    df = df[df['description'] != '']\n",
    "    #df.rename(columns = {'description' : 'project', 'bereich' : 'class'})\n",
    "    df['project'] = df['description']\n",
    "    df['label'] = df['bereich']\n",
    "    df.drop(['description', 'bereich'], axis=1, inplace=True)\n",
    "    df = df[df['label'] != 'IT/Bauingenieur']\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Für einen unserer Kunden aus dem Finanzdienstl...</td>\n",
       "      <td>Infr-Admin-Microsoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kann Profil leider nicht löschen.</td>\n",
       "      <td>IT/Consulting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Intelligence Analyst (m/w) - Tableau ...</td>\n",
       "      <td>Data-Sci-BI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Konzeption, Customizing sowie Softwareanpassun...</td>\n",
       "      <td>Infr-Admin-Linux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es sollen mehrere Automatisierungen mit ubot S...</td>\n",
       "      <td>IT/IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             project                 label\n",
       "0  Für einen unserer Kunden aus dem Finanzdienstl...  Infr-Admin-Microsoft\n",
       "1                  Kann Profil leider nicht löschen.         IT/Consulting\n",
       "2  Business Intelligence Analyst (m/w) - Tableau ...           Data-Sci-BI\n",
       "3  Konzeption, Customizing sowie Softwareanpassun...      Infr-Admin-Linux\n",
       "4  Es sollen mehrere Automatisierungen mit ubot S...                 IT/IT"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessedDataset = get_required_dataset(df_rawData)\n",
    "df_preprocessedDataset.shape\n",
    "df_preprocessedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Java Java EE/J2EE  Apache Tomcat HTML5  JSP, J...</td>\n",
       "      <td>Dev-Web-Fullstack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Projektbeschreibung \\n\\n                      ...</td>\n",
       "      <td>SW-Dev-Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We are recruiting for a  \\n \\nSenior SAP PP-DS...</td>\n",
       "      <td>ERP-SAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beginn: asap / Dauer: 6MM + / Ort: Frankfurt /...</td>\n",
       "      <td>SW-Dev-Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Projektbeschreibung \\n\\n                      ...</td>\n",
       "      <td>IT-Mgmt-Consulting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             project               label\n",
       "0  Java Java EE/J2EE  Apache Tomcat HTML5  JSP, J...   Dev-Web-Fullstack\n",
       "1  Projektbeschreibung \\n\\n                      ...       SW-Dev-Others\n",
       "2  We are recruiting for a  \\n \\nSenior SAP PP-DS...             ERP-SAP\n",
       "3  Beginn: asap / Dauer: 6MM + / Ort: Frankfurt /...       SW-Dev-Others\n",
       "4  Projektbeschreibung \\n\\n                      ...  IT-Mgmt-Consulting"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the data\n",
    "df_preprocessedDataset = df_preprocessedDataset.sample(frac=1.0)\n",
    "df_preprocessedDataset.reset_index(drop=True,inplace=True)\n",
    "df_preprocessedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Java Java EE/J2EE  Apache Tomcat HTML5  JSP, JSF, Open Source MySQL, Docker, CSS optional Automotive Know How Sichere Deutsch- sowie Englischkenntnisse Ihre Aufgaben Design, Entwicklung und Test bis hin zum Deployment maßgeschneiderter IT-Lösungen in der Automotive Branche Neubau einer Microservice-Applikation \"Technische Daten\" für eine Bereitstellungsplattform Kontakt Cegeka Deutschland GmbH Senta Ehrlich Martin-Behaim-Straße 22 63263 Neu-Isenburg Tel. +49 6102 8235 835 Fax +49 6102 8235 789 senta.ehrlich@cegeka.de'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessedDataset.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_data(text):\n",
    "    text = re.sub(r'(\\d)',' ',text.lower())\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to prepare 12168 projects documents: 0.37454417943954466 min\n"
     ]
    }
   ],
   "source": [
    "# Clean text and title and create new column \"tokenized\"\n",
    "t1 = time.time()\n",
    "df_preprocessedDataset['tokenized'] = df_preprocessedDataset['project'].apply(tokenization_data)\n",
    "t2 = time.time()\n",
    "print(\"Time taken to prepare\", len(df_preprocessedDataset), \"projects documents:\", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Java Java EE/J2EE  Apache Tomcat HTML5  JSP, J...</td>\n",
       "      <td>Dev-Web-Fullstack</td>\n",
       "      <td>[java, java, ee, j, ee, apache, tomcat, html, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Projektbeschreibung \\n\\n                      ...</td>\n",
       "      <td>SW-Dev-Others</td>\n",
       "      <td>[projektbeschreibung, melden, sie, sich, jetzt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We are recruiting for a  \\n \\nSenior SAP PP-DS...</td>\n",
       "      <td>ERP-SAP</td>\n",
       "      <td>[we, are, recruiting, for, a, senior, sap, pp,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beginn: asap / Dauer: 6MM + / Ort: Frankfurt /...</td>\n",
       "      <td>SW-Dev-Others</td>\n",
       "      <td>[beginn, asap, dauer, mm, ort, frankfurt, id, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Projektbeschreibung \\n\\n                      ...</td>\n",
       "      <td>IT-Mgmt-Consulting</td>\n",
       "      <td>[projektbeschreibung, melden, sie, sich, jetzt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             project               label  \\\n",
       "0  Java Java EE/J2EE  Apache Tomcat HTML5  JSP, J...   Dev-Web-Fullstack   \n",
       "1  Projektbeschreibung \\n\\n                      ...       SW-Dev-Others   \n",
       "2  We are recruiting for a  \\n \\nSenior SAP PP-DS...             ERP-SAP   \n",
       "3  Beginn: asap / Dauer: 6MM + / Ort: Frankfurt /...       SW-Dev-Others   \n",
       "4  Projektbeschreibung \\n\\n                      ...  IT-Mgmt-Consulting   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [java, java, ee, j, ee, apache, tomcat, html, ...  \n",
       "1  [projektbeschreibung, melden, sie, sich, jetzt...  \n",
       "2  [we, are, recruiting, for, a, senior, sap, pp,...  \n",
       "3  [beginn, asap, dauer, mm, ort, frankfurt, id, ...  \n",
       "4  [projektbeschreibung, melden, sie, sich, jetzt...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/jupyter/Icxa/projectfinder_analytics/ML/USL\n"
     ]
    }
   ],
   "source": [
    "# load nltk's German and english stopwords'\n",
    "currDir = os.getcwd()\n",
    "print(currDir)\n",
    "if \"USL\" not in currDir:\n",
    "    dataDir = os.path.join(currDir,  \"ML\", \"USL\", \"data\")\n",
    "else: \n",
    "    dataDir = os.path.join(currDir,  \"data\")\n",
    "with open(os.path.join(dataDir, 'german_stopwords_full.txt'), 'r') as f:\n",
    "    stopwords_germ = f.read().splitlines()\n",
    "stopwords_eng = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#german cities\n",
    "if \"USL\" not in currDir:\n",
    "    from ML.USL.bundeslander import Baden_Württemberg, Bayern, Berlin, Brandenburg, Bremen, Hamburg, Hessen, Mecklenburg_Vorpommern, Niedersachsen, Nordrhein_Westfalen, Rheinland_Pfalz, Saarland, Sachsen, Sachsen_Anhalt, Schleswig_Holstein, Thüringen, Ausland\n",
    "else:\n",
    "    from bundeslander import Baden_Württemberg, Bayern, Berlin, Brandenburg, Bremen, Hamburg, Hessen, Mecklenburg_Vorpommern, Niedersachsen, Nordrhein_Westfalen, Rheinland_Pfalz, Saarland, Sachsen, Sachsen_Anhalt, Schleswig_Holstein, Thüringen, Ausland\n",
    "\n",
    "All = Baden_Württemberg + Bayern + Berlin + Brandenburg + Bremen +Hamburg + Hessen + Mecklenburg_Vorpommern + Niedersachsen + Nordrhein_Westfalen + Rheinland_Pfalz + Saarland + Sachsen + Sachsen_Anhalt + Schleswig_Holstein + Thüringen + Ausland\n",
    "cities = list(set([city.lower() for city in All]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['januar', 'january', 'februar', 'february', 'märz', 'march', 'april', 'mai', 'may', 'juni', 'june', 'juli', 'july', 'august', 'september', 'oktober', 'october', 'november', 'dezember', 'december']\n"
     ]
    }
   ],
   "source": [
    "months = ['Januar', 'January','Februar', 'February', 'März', 'March', 'April', 'Mai', 'May', 'Juni', 'June', 'Juli', \n",
    "          'July', 'August', 'September', 'Oktober', 'October', 'November', 'Dezember', 'December']\n",
    "months = [month.lower() for month in months]\n",
    "print(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n"
     ]
    }
   ],
   "source": [
    "stopwords_manual = [line.rstrip('\\n') for line in open(os.path.join(dataDir, 'stopwords_manual.txt'))]\n",
    "print(len(stopwords_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13240"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_all = list(set(stopwords_germ + stopwords_eng + stopwords_manual + cities + months))\n",
    "len(stopwords_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "stopwords_add = []\n",
    "stopwords_add = list(set(stopwords_add + stopwords_manual))\n",
    "checker = list(set(stopwords_germ + stopwords_eng + cities + months))\n",
    "stopwords_add.sort()\n",
    "with open('stopwords_manual.txt', 'w') as f:\n",
    "    for item in stopwords_add:\n",
    "        if item not in checker:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_manual = [line.rstrip('\\n') for line in open('stopwords_manual.txt')]\n",
    "print(len(stopwords_manual))\n",
    "\n",
    "stopwords_all = list(set(stopwords_germ + stopwords_eng + stopwords_manual + cities + months))\n",
    "len(stopwords_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    return [word for word in words if word not in stopwords_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "df_preprocessedDataset['tokenized_wo_stopwords'] = df_preprocessedDataset['tokenized'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(df_preprocessedDataset['tokenized'].tolist(), min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "def make_bigrams(text):\n",
    "    return bigram_mod[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "df_preprocessedDataset['tokenized_w_bigrams'] = df_preprocessedDataset['tokenized_wo_stopwords'].apply(make_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all bigrams\n",
    "all_lines = df_preprocessedDataset['tokenized_w_bigrams'].tolist()\n",
    "all_bigrams = list(set([word for words in all_lines for word in words if '_' in word]))\n",
    "all_bigrams.sort()\n",
    "print(len(all_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_bigrams_Approach_2.txt', 'w') as f:\n",
    "    for item in all_bigrams:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "stemmer_own = {\n",
    "    \n",
    "    'abgeschlossen': 'abgeschlossen',\n",
    "    'admin': 'administration',  \n",
    "    'verwaltung': 'administration',\n",
    "    'architektur' : 'architekture',\n",
    "    'agil' : 'agile',\n",
    "    'analys': 'analyst',\n",
    "    'app': 'application',\n",
    "    'anwend' : 'application',\n",
    "    'automat': 'automate',\n",
    "   \n",
    "    \n",
    "    'consultant' : 'berater',\n",
    "    'berat': 'berater',\n",
    "    'bereich' : 'bereich',\n",
    "    'cisco': 'cisco',\n",
    "    'konzept' : 'concept',\n",
    "    'container': 'containerization',\n",
    "    'contin': 'continuous',\n",
    "    'zertifi' : 'certificate',\n",
    "    'certifi' : 'certificate',\n",
    "    'design' : 'design',\n",
    "    'engineer' : 'engineer',\n",
    "    'ingenieur'  : 'engineer',\n",
    "    'entwick': 'entwicklung',\n",
    "    'develop': 'entwicklung',\n",
    "    'device':'device',\n",
    "    'program': 'entwicklung',\n",
    "    'entwickler' : 'entwicklung',\n",
    "    \n",
    "    'extern': 'external',\n",
    "    'framework': 'framework',\n",
    "    'fix': 'fix',\n",
    "    'globalen': 'global',\n",
    "    'install' : 'install',\n",
    "    'schnittstell': 'interface',\n",
    "    'implement' : 'implementation', \n",
    "    'infrastr' : 'infrastructure',\n",
    "    'informati' : 'informatik',\n",
    "    'intern': 'internal',\n",
    "    'integriert' : 'integrate',\n",
    "    'konfigur': 'konfigure',\n",
    "    'manage' : 'management',\n",
    "    'method' : 'method',\n",
    "    'überwach' : 'monitoring',\n",
    "    'mobil': 'mobil',\n",
    "    'betrieb' : 'operation',\n",
    "    'künstliche': 'künstliche',\n",
    "    'notebook': 'notebooks',\n",
    "    'read':'read',\n",
    "    'write':'write',\n",
    "    'relational':'relational',\n",
    "    'master':'master',\n",
    "    'script':'script',\n",
    "    'skript':'skript',\n",
    "    'skale':'scale',\n",
    "    \n",
    "    'operat' : 'operation',\n",
    "    'operie' : 'operation',\n",
    "    'vorschläg' : 'option',\n",
    "    'plattform' : 'platform',\n",
    "    'projec' : 'project',\n",
    "    'prozess' : 'process',\n",
    "    'process' : 'process',\n",
    "    'bearbeitung' : 'process',\n",
    "    'scrum': 'scrum',\n",
    "    'softwar': 'software',\n",
    "    'spezifi' :'specification',\n",
    "    'specifi' :'specification',\n",
    "    'unterstützt' : 'support',\n",
    "    'support' : 'support',\n",
    "    'system': 'system',\n",
    "    'anfoder': 'requirement',\n",
    "    'tech' : 'tech',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Normalize, tokenize and stem text string\n",
    "    \n",
    "    Args:\n",
    "    text: string. String containing message for processing\n",
    "       \n",
    "    Returns:\n",
    "    cleaned: list of strings. List containing normalized and stemmed word tokens\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        text = re.sub(r'(\\d)',' ',text.lower())\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens_cleaned = [word for word in tokens if word not in stopwords_all and len(word) > 1]\n",
    "        cleaned = []\n",
    "        stemmer_keys = list(stemmer_own.keys())\n",
    "        for word in tokens_cleaned:\n",
    "            for stemmer_key in stemmer_keys:\n",
    "                if stemmer_key in word:\n",
    "                    stemmed_word = stemmer_own[stemmer_key]\n",
    "                    cleaned.append(stemmed_word)\n",
    "                    break\n",
    "            else:\n",
    "                cleaned.append(word)\n",
    "  \n",
    "                \n",
    "\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text and title and create new column \"tokenized\"\n",
    "t1 = time.time()\n",
    "df_preprocessedDataset['tokenized'] = df_preprocessedDataset['project'].apply(tokenize)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time taken to prepare\", len(df_preprocessedDataset), \"projects documents:\", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessedDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list containing all the words in a dataframe\n",
    "all_words_df = [word for item in list(df_preprocessedDataset['tokenized']) for word in item]\n",
    "\n",
    "# Use nltk fdist to get a frequency distribution of all words\n",
    "fdist_words = FreqDist(all_words_df)\n",
    "print(len(fdist_words)) # number of unique words\n",
    "print(type(fdist_words))\n",
    "\n",
    "#print(fdist_words.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unique_words = len(fdist_words)\n",
    "sorted_freqDist_words = fdist_words.most_common()\n",
    "maxFreq = sorted_freqDist_words[0][1]\n",
    "print(maxFreq)\n",
    "freq_values = [sorted_freqDist_words[i][1] for i in range(total_unique_words)]\n",
    "avgFreq = np.mean(freq_values)\n",
    "print(avgFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering words with frequency of 100 or more\n",
    "top_words = [sorted_freqDist_words[i][0] for i in range(total_unique_words) if sorted_freqDist_words[i][1] >= 100]\n",
    "print(len(top_words))\n",
    "#print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def most_appeared(text):\n",
    "    return [word for word in text if word in top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the words in tokenized column to the words with frequency more than 100. \n",
    "df_preprocessedDataset['tokenized'] = df_preprocessedDataset['tokenized'].apply(most_appeared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessedDataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep articles with more than 10 tokens, otherwise too short\n",
    "df_preprocessedDataset = df_preprocessedDataset[df_preprocessedDataset['tokenized'].map(len) >= 10]\n",
    "# make sure all tokenized items are lists\n",
    "df_preprocessedDataset = df_preprocessedDataset[df_preprocessedDataset['tokenized'].map(type) == list]\n",
    "df_preprocessedDataset.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df_preprocessedDataset), \"articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask of binary values to split into train and test\n",
    "msk = np.random.rand(len(df_preprocessedDataset)) < 0.9960\n",
    "msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_preprocessedDataset[msk]\n",
    "train_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "test_df = df_preprocessedDataset[~msk]\n",
    "test_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_lda(data, n=10):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize\n",
    "    \"\"\"\n",
    "    num_topics = n\n",
    "    chunksize = 300\n",
    "    dictionary = corpora.Dictionary(data['tokenized'])\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=2)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on \", len(df_preprocessedDataset), \"documents: \", (t2-t1)/60, \"min\")\n",
    "    return dictionary,corpus,lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary,corpus,lda = train_lda(train_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.save('LDA_Approach_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "model =  models.LdaModel.load('LDA_Approach_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all topics\n",
    "model.show_topics(num_topics=20, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary_LDA_A1', 'wb') as output:\n",
    "    pickle.dump(dictionary, output)\n",
    "    \n",
    "with open('corpus_LDA_A1', 'wb') as output:\n",
    "    pickle.dump(corpus, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath(\"model\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "pickle.dump(lda, open('model_LDA_A1', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a potentially pretrained model from disk.\n",
    "lda2 = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_topics method shows the the top num_words contributing to num_topics number of random topics\n",
    "lda.show_topics(num_topics=13, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_id in range (2):\n",
    "    print(\"TopicID: \" + str(t_id))\n",
    "    topics = lda.show_topic(topicid=t_id, topn=20)\n",
    "    for topic in topics:\n",
    "        print(topic[0] + \": \" + str(topic[1]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random project from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select an article at random from train_df\n",
    "random_index = int(np.random.randint(len(train_df), size=[1, 1]))\n",
    "print(random_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_check = train_df.iloc[random_index,2]\n",
    "bow = dictionary.doc2bow(data_to_check)\n",
    "doc_distribution = np.array([topic[1] for topic in lda.get_document_topics(bow=bow)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.iloc[random_index,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_distribution)\n",
    "print(len(doc_distribution))\n",
    "np.argsort(-doc_distribution)[:3]\n",
    "print(doc_distribution)\n",
    "print(len(doc_distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of topic distribution for this document\n",
    "def plot_topic_dist(doc_distr, index):\n",
    "    \"\"\"\n",
    "    This function plots the topic distrubtion for a given document\n",
    "    It takes two parameters\n",
    "    (1) doc_distr = type: list of floats, list of topic probability distribution in a document\n",
    "    (2) index = type: int, index number of document to plot\n",
    "    We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12,8));\n",
    "    # the histogram of the data\n",
    "    patches = ax.bar(np.arange(len(doc_distr)), doc_distr)\n",
    "    ax.set_xlabel('Topic ID', fontsize=15)\n",
    "    ax.set_ylabel('Topic Probability Score', fontsize=15)\n",
    "    ax.set_title(\"Topic Distribution for Project in Index \" + str(index), fontsize=20)\n",
    "    ax.set_xticks(range(0,10))\n",
    "    x_ticks_labels = ['ERP/SAP','SW_Dev/Web','IT_App_Mgr/SW_Dev_Arch','SW_Dev/DevOps','Sys_Admin/Support', 'IT_Admin_SW/Oracle/Ops','Data/Ops','IT_Process_Mgr/Consultant', 'MS_DEV/Admin','Business_Analyst/Consulting']\n",
    "    ax.set_xticklabels(x_ticks_labels, rotation='vertical', fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_dist(doc_distribution, random_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  models.LdaModel.load('LDA_Approach_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_distribution1 = np.array([topic[1] for topic in lda_model.get_document_topics(bow=bow)])\n",
    "labels = np.argmax(doc_distribution1)\n",
    "print(doc_distribution1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
